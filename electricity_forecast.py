# -*- coding: utf-8 -*-
"""95474_Ewa_Zawadzka_ES_Project_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jPjZ7O6N6ajwobbUIDfvOiQ5RtzZAkvj

# 1. Libraries
"""

import numpy as np
import pandas as pd
from matplotlib import pyplot as plt

"""## 1.1.Defining charts:"""

# LINE CHART
def choose_grid(nr):
    return nr // 4 + 1, 4

def line_chart(ax: plt.Axes, series: pd.Series, title: str, xlabel: str, ylabel: str, percentage=False):
    ax.set_title(title)
    ax.set_xlabel(xlabel)
    ax.set_ylabel(ylabel)
    if percentage:
        ax.set_ylim(0.0, 1.0)
    ax.plot(series)

# (rows, cols) = choose_grid(data.shape[1])
# plt.figure()
# fig, axs = plt.subplots(rows, cols, figsize=(cols*5, rows*5))
# i, j, n = 0, 0, 0

# for col in data:
#     line_chart(axs[i, j], data[col], col, 'date', col)
#     n = n + 1
#     i, j = (i + 1, 0) if n % cols == 0 else (i, j + 1)
# fig.tight_layout()
# plt.show()

# MULTIPLE LINE CHART
def multiple_line_chart(ax: plt.Axes, xvalues: list, yvalues: dict, title: str, xlabel: str, ylabel: str, percentage=False):
    legend: list = []
    ax.set_title(title)
    ax.set_xlabel(xlabel)
    ax.set_ylabel(ylabel)
    if percentage:
        ax.set_ylim(0.0, 1.0)

    for name, y in yvalues.items():
        ax.plot(xvalues, y)
        legend.append(name)
    ax.legend(legend, loc='best', fancybox = True, shadow = True)   
    
# plt.figure(figsize=(12,4))
# two_series = {'Phosphate': data['Phosphate'], 'Orthophosphate': data['Orthophosphate']}
# multiple_line_chart(plt.gca(), data.index, two_series, 'Phosphate and Orthophosphate values', 'date', '')
# plt.show() 


#  BART CHART
def bar_chart(ax: plt.Axes, xvalues: list, yvalues: list, title: str, xlabel: str, ylabel: str, percentage=False):
    ax.set_title(title)
    ax.set_xlabel(xlabel)
    ax.set_ylabel(ylabel)
    ax.set_xticklabels(xvalues, rotation=90, fontsize='small')
    if percentage:
        ax.set_ylim(0.0, 1.0)
    ax.bar(xvalues, yvalues, edgecolor='grey')
  
# plt.figure()    
# counts = data['season'].value_counts()
# bar_chart(plt.gca(), counts.index, counts.values, 'season distribution', 'season', 'frequency')
# plt.show()

#  MULTIPLE BAR CHART
def multiple_bar_chart(ax: plt.Axes, xvalues: list, yvalues: dict, title: str, xlabel: str, ylabel: str, percentage=False):

    ax.set_title(title)
    ax.set_xlabel(xlabel)
    ax.set_ylabel(ylabel)
    x = np.arange(len(xvalues))  # the label locations
    ax.set_xticks(x)
    ax.set_xticklabels(xvalues, fontsize='small')
    if percentage:
        ax.set_ylim(0.0, 1.0)
    width = 0.8  # the width of the bars
    step = width / len(yvalues)
    k = 0
    for name, y in yvalues.items():
        ax.bar(x + k * step, y, step, label=name)
        k += 1
    ax.legend(loc='lower center', ncol=len(yvalues), bbox_to_anchor=(0.5, -0.2), fancybox = True, shadow = True)    

# two_series = {'river_depth': data['river_depth'].value_counts().sort_index(), 
#               'fluid_velocity': data['fluid_velocity'].value_counts().sort_index()}
# plt.figure()
# multiple_bar_chart(plt.gca(), ['high', 'low', 'medium'], two_series, '', '', 'frequency')
# plt.show()

"""# 2. Uploading files"""

# connecting with a drive
from google.colab import drive
drive.mount('/content/drive')

# uploading 5 files

# central building data
path1 = '/content/drive/My Drive/Colab Notebooks/Energy Services IST/IST_Central_Pav_2017_Ene_Cons.csv'
df1_original = pd.read_csv(path1)
path2 = '/content/drive/My Drive/Colab Notebooks/Energy Services IST/IST_Central_Pav_2018_Ene_Cons.csv'
df2_original = pd.read_csv(path2)
# weather data
path3 = '/content/drive/My Drive/Colab Notebooks/Energy Services IST/IST_meteo_data_2017_2018_2019.csv'
meteo_original = pd.read_csv(path3)
# univeristy and holiday calendar data
path4 = '/content/drive/My Drive/Colab Notebooks/Energy Services IST/cal_2017.csv'
cal_2017_original = pd.read_csv(path4, sep =';')
path5 = '/content/drive/My Drive/Colab Notebooks/Energy Services IST/cal_2018.csv'
cal_2018_original = pd.read_csv(path5, sep =';')

"""# 3. Content identification & cleaning

## 3.1. Quick outlook
"""

pd.set_option("display.max_columns", None)
 
# print(df1_original.describe)
# print(df2_original.dtypes)
# print(meteo_original.head())

# print(df1_original.shape)
# print(df2_original.shape)
# print(meteo_original.shape)

# print(df1_original.columns)
# print(df2_original.columns)
# print(meteo_original.columns)

"""## 3.2. Quick cleaning
In this chapter:
* modification the date into single column 'Date' in each dataset
* hourly average from weather datapoint 
* merging datasets together (df1, df2, meteo)
* changing the type of 'Date' for datetime
* merging cateories from university calendar into 4 groups and give them the weight from 1 to 4 regards the assumed impact on power consumption

"""

##deleting 'Date_end' column
df1_date = df1_original.drop('Date_end', axis=1)
df2_date = df2_original.drop('Date_end', axis=1)

##changing the names of the columns for 'Date'
df1_date = df1_date.rename(columns = {"Date_start":"Date"}) 
df2_date = df2_date.rename(columns = {"Date_start":"Date"}) 
meteo_all = meteo_original.rename(columns={'yyyy-mm-dd hh:mm:ss':'Date'})

#merging the years 2017 and 2018
data = pd.concat([df1_date, df2_date])
data.reset_index(inplace = True, drop = True) 

#deleting weather 2019 from the file 
meteo_new = meteo_all[~meteo_all['Date'].str.contains('2019', na=False)]
meteo_new.reset_index(inplace = True, drop = True) 

#merging university calendar 2017 and 2018
cal_uni = pd.concat([cal_2017_original, cal_2018_original])
cal_uni.reset_index(inplace = True, drop = True)

data_timechange = data.copy()
meteo_timechange = meteo_new.copy()

##changing 'Date' type to the datetime
data_timechange['Date'] = pd.to_datetime(data_timechange.Date, dayfirst=True)
meteo_timechange['Date'] = pd.to_datetime(meteo_timechange.Date, dayfirst=False)

#squizzing the weather data, doing the hourly mean    
meteo_mean = meteo_timechange.resample('h', on='Date').mean().dropna(how='all')                # ('Date' column become an index here !)
meteo_mean.reset_index(inplace = True, drop = False)                                           # 'Date' as a column again

"""### Attention
This part was done, after first iteration of all data exploration and cleaning. Using the heat map below, 
the missing records in weather datates were detected. In order to minimalize them, the following stategy was developed: 
The missing data in one year was imputed by the data from the other year. 
In order to allow such move, a lot of transformation nedded to be done.
"""

# creating separate files for weather 2017 and 2018
meteo_2017 = meteo_new.copy()[~meteo_new['Date'].str.contains('2018', na=False)]
meteo_2018 = meteo_new.copy()[~meteo_new['Date'].str.contains('2017', na=False)]
# chaning the date to datetime
meteo_2017['Date'] = pd.to_datetime(meteo_2017.Date, dayfirst=False)
meteo_2018['Date'] = pd.to_datetime(meteo_2018.Date, dayfirst=False)
# doing the hourly mean
meteo_mean_2017 = meteo_2017.resample('h', on='Date').mean().dropna(how='all') 
meteo_mean_2017.reset_index(inplace = True, drop = False)      
meteo_mean_2018 = meteo_2018.resample('h', on='Date').mean().dropna(how='all')  
meteo_mean_2018.reset_index(inplace = True, drop = False)    
# creating separate columns with month and days and hour
meteo_mean_2017['Month'] = pd.DatetimeIndex(meteo_mean_2017['Date']).month
meteo_mean_2017['Day'] = pd.DatetimeIndex(meteo_mean_2017['Date']).day
meteo_mean_2017['Hour'] = pd.DatetimeIndex(meteo_mean_2017['Date']).hour
meteo_mean_2018['Month'] = pd.DatetimeIndex(meteo_mean_2018['Date']).month
meteo_mean_2018['Day'] = pd.DatetimeIndex(meteo_mean_2018['Date']).day
meteo_mean_2018['Hour'] = pd.DatetimeIndex(meteo_mean_2018['Date']).hour
# making Month and Day and Hour as a string with leading zeros
meteo_mean_2017.Month = meteo_mean_2017.Month.astype(str).str.zfill(2)
meteo_mean_2017.Day = meteo_mean_2017.Day.astype(str).str.zfill(2)
meteo_mean_2017.Hour = meteo_mean_2017.Hour.astype(str).str.zfill(2)
meteo_mean_2018.Month = meteo_mean_2018.Month.astype(str).str.zfill(2)
meteo_mean_2018.Day = meteo_mean_2018.Day.astype(str).str.zfill(2)
meteo_mean_2018.Hour = meteo_mean_2018.Hour.astype(str).str.zfill(2)
# creating a column with the code of: month, day,hour
meteo_mean_2017['period'] = meteo_mean_2017['Month'] + meteo_mean_2017['Day'] + meteo_mean_2017['Hour']
meteo_mean_2018['period'] = meteo_mean_2018['Month'] + meteo_mean_2018['Day'] + meteo_mean_2018['Hour']
# checking the  differences in datasets based on the code above
idx1 = pd.Index(meteo_mean_2017['period'])
idx2 = pd.Index(meteo_mean_2018['period'])
this_is_in_2018 = idx2.difference(idx1)
this_is_in_2017 = idx1.difference(idx2)

# print(this_is_in_2018)
# print(len(this_is_in_2018))

# print(this_is_in_2017)
# print(len(this_is_in_2017))

# print(meteo_mean_2017.shape)
# print(meteo_mean_2018.shape)
# print(meteo_mean_2017.head())

# print(meteo_mean_2018.loc[meteo_mean_2018['period'] == '101323' ])
# print(meteo_mean_2017.loc[meteo_mean_2017['period'] == '10140' ])
# print(meteo_mean_2017.loc[6065 ])

# creating datasets that are in one year and are missing in another
lack_in_2018 = pd.DataFrame()
for col1 in this_is_in_2017:
    rzad1 = meteo_mean_2017.loc[meteo_mean_2017['period'] == col1 ]
    lack_in_2018 = lack_in_2018.append(rzad1, ignore_index=True)

lack_in_2017 = pd.DataFrame()
for col2 in this_is_in_2018:
    rzad2 = meteo_mean_2018.loc[meteo_mean_2018['period'] == col2 ]
    lack_in_2017 = lack_in_2017.append(rzad2, ignore_index=True)

# merging the datasets with lacking data from other year
full_2017 = pd.concat([meteo_mean_2017, lack_in_2017])
full_2018 = pd.concat([meteo_mean_2018, lack_in_2018])

# adding the column of the year to create new date later
full_2017['Year'] = 2017
full_2018['Year'] = 2018

# changing the type of month, hay, and hour to be sure there won't be an error with to_datetime (if the values are object)
full_2017.Month = full_2017.Month.astype(int)
full_2017.Day = full_2017.Day.astype(int)
full_2017.Hour = full_2017.Hour.astype(int)
full_2018.Month = full_2018.Month.astype(int)
full_2018.Day = full_2018.Day.astype(int)
full_2018.Hour = full_2018.Hour.astype(int)

# repearing the stuff with the year in the date
full_2017_date = full_2017.copy()
full_2018_date = full_2018.copy()

# deleting old 'Date' column - to be sure no errors with creating a new one
full_2017_date = full_2017_date.drop('Date', axis = 1)
full_2018_date = full_2018_date.drop('Date', axis = 1)

# creating new datetime based on the columns ['Year','Month', 'Day' ,'Hour']
full_2017_date['Date'] = pd.to_datetime(full_2017_date[['Year','Month', 'Day' ,'Hour']])
full_2018_date['Date'] = pd.to_datetime(full_2018_date[['Year','Month', 'Day' ,'Hour']])

# sorting by date
full_2017_sort = full_2017_date.sort_values(by = 'Date', ascending = True)
full_2018_sort = full_2018_date.sort_values(by = 'Date', ascending = True)

# deleting columns that were needed only for previeous operations ('Hour' column stay , cause it predicted to be an important feature)
full_2017_new = full_2017_sort.drop(columns = ['Year','Month', 'Day', 'period'])
full_2018_new = full_2018_sort.drop(columns = ['Year','Month', 'Day', 'period'])

#merging two hopefully completed years into one dataset
meteo_full = pd.concat([full_2017_new, full_2018_new])

# reset indexes
meteo_full.reset_index(inplace = True, drop = True)

"""The end of weather datasets transformations.

### Univeristy calendar + holidays
"""

# creating the dataset (extra rows from columns) with time ( original dataset has column for each hour in order to make it compatible with other datasets)
new_data = []
for index,row in cal_uni.iterrows(): 
    date = str(row['Date']) 
    for hour in range(0,24): 
        Date = date
        Hour = str(hour)
        Category = row[str(hour)] 
        new_row = [Date, Hour, Category] 
        new_data.append(new_row) 

cal_uni_date = pd.DataFrame(data = new_data, columns = ['Date','Hour', 'Category'])

# creating separate columns with month and days and hour
cal_uni_date['Year'] = pd.DatetimeIndex(cal_uni_date['Date']).year
cal_uni_date['Month'] = pd.DatetimeIndex(cal_uni_date['Date']).month
cal_uni_date['Day'] = pd.DatetimeIndex(cal_uni_date['Date']).day

# changing the 'Hour' type for int
cal_uni_date.Hour = cal_uni_date.Hour.astype(int)
# setting the proper 'Date'
cal_uni_date['Date'] = pd.to_datetime(cal_uni_date[['Year', 'Month','Day','Hour']], dayfirst=False)
# deleting the columns not needed anymore
cal_uni_date = cal_uni_date.drop(['Hour','Year','Month','Day'], axis =1)

# merging categories into 4 groups
cal_uni_cat = cal_uni_date.replace('holiday', 'freedom')
cal_uni_cat = cal_uni_cat.replace(['ex.prep','exam'],'exams')
cal_uni_cat = cal_uni_cat.replace(['host week','maybe'],'others')
cal_uni_cat = cal_uni_cat.replace('class','classes')
# filling NaN with 'freedom'
cal_uni_cat = cal_uni_cat.fillna('freedom')

# print('Class', cal_uni_cat['Class'].unique())

#changing the cateories to number (assuming that 1 corresponds to the smallest consumption and 4 the biggest)
cal_uni_float= cal_uni_cat.replace('freedom', 100)
cal_uni_float= cal_uni_float.replace('others', 200)
cal_uni_float= cal_uni_float.replace('exams', 300)
cal_uni_float= cal_uni_float.replace('classes', 400)

"""### The rest:
two options to run the model with filled weather dataset or the 'original' without some records

"""

# creating the weather dataset without imputed missing records
meteo_full_not_complete = meteo_mean.copy()
meteo_full_not_complete ['Hour'] = pd.DatetimeIndex(meteo_full_not_complete ['Date']).hour

# data_all = pd.merge(data_timechange, meteo_full_not_complete, how = 'left', on = 'Date')           # old meteo with missing records

data_all = pd.merge(data_timechange, meteo_full, how = 'left', on = 'Date')             # new meteo with imputed records
data_all = pd.merge(data_all, cal_uni_float, how = 'left',  on = 'Date')
data_all = pd.merge(data_all, cal_uni_cat,  how = 'left',  on = 'Date')

# cleating new feature - WeekDay
data_all['WeekDay'] = pd.to_datetime(data_all['Date'], format='%d-%m-%Y %H:%M').dt.weekday

"""## 3.3. Missing values:
221 rows with missing weather data were droped
"""

# defining function for returning the rows with missing values
def nans(df): return df[df.isnull().any(axis=1)]

"""###Identification"""

#plotting the nr. of missing values
fig = plt.figure(figsize=(10,7))
mv1 = {}
for var1 in data_all:
    mv1[var1] = data_all[var1].isna().sum()
    bar_chart(plt.gca(), mv1.keys(), mv1.values(), 'Number of missing values per variable', var1, 'nr. missing values')
    
fig.tight_layout()
plt.show()
print(mv1.keys(), mv1.values())

"""###Treatement"""

# deleting 'Duration' column & rows that are totally empty
data_clear = data_all.dropna(axis=1, how='all', thresh=None, subset=None, inplace=False)
data_clear = data_clear.dropna(axis=0, how='all', thresh=None, subset=None, inplace=False)
# creatin dataset where weather has missing values in order to be aware what dates are missing after removing the rows with missing data
missing = nans(data_clear)
# the are still 221 missing values in weather data. This is not a significant nr, so I decided to drop that records:
data_clear = data_clear.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)
data_clear.reset_index(inplace = True, drop = True)

# print('rain_day', data_clear['rain_day'].unique())

# pd.set_option("display.max_rows", None)
# missing

# nr. of missing values after treatement
data_clear.isnull().sum()

# pd.set_option("display.max_row", None)
# print(nans(data_clear))
# print(data_clear.tail())
# print(data_clear.shape)
# print(data_clear.loc[data_clear['Date'] == '2018-11-04 01:00:00' ])

"""# 4. Data exploration
* checking the distribution
* outliers detection and treatement
* heat map
"""

data_de = data_clear

"""## 4.1. Distribution
Tt is all comented, because it need more coomputational power and do not provide us with important information. 
"""

import seaborn as sns

# # distribution for each attribute (commented cause need more computational power than rest and it is not so important, just for the visualization)

# columns = data_de.select_dtypes(include='number').columns
# rows, cols = 4, 5
# plt.figure()
# fig, axs = plt.subplots(rows, cols, figsize=(cols*4, rows*4), squeeze=False)
# i, j = 0, 0
# for n in range(len(columns)):
#     axs[i, j].set_title('Histogram with trend for %s'%columns[n])
#     axs[i, j].set_ylabel("probability")
#     sns.distplot(data_de[columns[n]].dropna().values, norm_hist=True, ax=axs[i, j], axlabel=columns[n])
#     i, j = (i + 1, 0) if (n+1) % cols == 0 else (i, j + 1)
# fig.tight_layout()
# plt.show()

"""## 4.2. Outliers

### Sorting
Sorting the data did not show any obvious outliers. In order to take a closer look, the plots of 'Power_Ah' and 'temp_C' were created. Still the outliers were not detected. However in order to prevent any outliers in potentional new dataset the function of removing outliers were built. The treshold was set for 10% below and above the extreme values.
"""

# # sorting as a first attempt to detect outliers 
# data_sort_Ah = data_de.sort_values(by = 'Total_Ah', ascending = False)
# data_sort_temp = data_de.sort_values(by = 'temp_C', ascending = False)

# print(data_sort_Ah.head())
# print(data_sort_Ah.tail())
# print(data_sort_temp.head())
# print(data_sort_temp.tail())

import matplotlib.ticker as ticker 

#changing index for the Date for plotting
data_plot_time = data_de.copy (deep =True)
data_plot_time = data_plot_time.set_index ('Date', drop = False)

"""### Charts

Power:
"""

fig1, ax = plt.subplots() 
fig1.set_size_inches(25,10) 

ax.xaxis.set_major_locator (ticker.MultipleLocator(250))
ax.xaxis.set_tick_params (which = 'major', pad = 5, labelrotation = 50)

plot_start = '2017-01-01 00:00:00' 
plot_end = '2018-12-31 23:00:00'

# #closer look for the smallest values  -- doesn't look suspicious
# plot_start = '2017-07-24 00:00:00' 
# plot_end = '2017-08-16 23:00:00'

# plot_start = '2018-10-24 00:00:00' 
# plot_end = '2018-12-15 23:00:00'

# Create a list of labels:
s = data_plot_time ['Date'] [plot_start : plot_end]
b = s.dt.strftime ('%Y-%b-%d-%H h')                          

y = data_plot_time ['Power_kW'][plot_start : plot_end]                       

plt.plot (b, y, '-o', color = 'blue', markersize = 10, linewidth = 1, markerfacecolor = 'red',  markeredgecolor = 'black', markeredgewidth = 3)

"""Temperature:"""

fig1, ax = plt.subplots() 
fig1.set_size_inches(25,10) 

ax.xaxis.set_major_locator (ticker.MultipleLocator(250))
ax.xaxis.set_tick_params (which = 'major', pad = 5, labelrotation = 50)

plot_start = '2017-01-01 00:00:00' 
plot_end = '2018-12-31 23:00:00'

# ploting two years in one chart in order to verify if the imputation of records worked  - working only when do not delete the 221 rows with missing data
# plot_start1 = '2017-10-14 23:00:00' 
# plot_end1 = '2017-11-30 23:00:00'

# plot_start2 = '2018-10-14 23:00:00' 
# plot_end2 = '2018-11-30 23:00:00'

s1 = data_plot_time ['Date'] [plot_start : plot_end]
b1 = s1.dt.strftime ('%Y-%b-%d-%H h')          

y1 = data_plot_time ['temp_C'][plot_start : plot_end]                      #red                
# y2 = data_plot_time ['temp_C'][plot_start2 : plot_end2]                  #gray
# y3 = data_plot_time ['rain_mm/h'][plot_start1 : plot_end1]               #blue              


plt.plot (b1, y1, '-o', color = 'red', markersize = 10, linewidth = 1, markerfacecolor = 'red',  markeredgecolor = 'black', markeredgewidth = 2)
# plt.plot (b1, y2, '-o', color = 'gray', markersize = 5, linewidth = 1, markerfacecolor = 'gray',  markeredgecolor = 'black', markeredgewidth = 2)
# plt.plot (b1, y3, '-o', color = 'blue', markersize = 7, linewidth = 1, markerfacecolor = 'blue',  markeredgecolor = 'black', markeredgewidth = 2)

"""#### Plotting the Categories from university and holiday calendar - imho. doesn't gives us any valuable information"""

fig1, ax = plt.subplots() 
fig1.set_size_inches(25,10) 

ax.xaxis.set_major_locator (ticker.MultipleLocator(250))
ax.xaxis.set_tick_params (which = 'major', pad = 5, labelrotation = 50)

plot_start = '2017-01-01 00:00:00' 
plot_end = '2017-03-31 23:00:00'

s = data_plot_time ['Date'] [plot_start : plot_end]
b = s.dt.strftime ('%Y-%b-%d-%H h')           

y = data_plot_time ['Power_kW'][plot_start : plot_end] 
y1 = data_plot_time ['Category_x'][plot_start : plot_end]                    #red                
# y2 = data_plot_time ['temp_C'][plot_start2 : plot_end2]                    #gray
# y3 = data_plot_time ['rain_mm/h'][plot_start1 : plot_end1]                 #blue              


plt.plot (b, y, '-o', color = 'gray', markersize = 7, linewidth = 1, markerfacecolor = 'gray',  markeredgecolor = 'black', markeredgewidth = 2)
plt.plot (b, y1, '-o', color = 'blue', markersize = 3, linewidth = 0.01, markerfacecolor = 'blue',  markeredgecolor = 'black', markeredgewidth = 0.05)
# plt.plot (b1, y3, '-o', color = 'blue', markersize = 7, linewidth = 1, markerfacecolor = 'blue',  markeredgecolor = 'black', markeredgewidth = 2)

# 100 - 'freedom'; 200 - 'others'; 300 - 'exams'; 400 'classes'

"""### Removing outliers  - only for new datasets"""

# defining the function for creating the dataset with removed outliers given treshold
def without_outlier_down(df, col_name, threshold):
  return df[df[col_name] > threshold]

def without_outlier_up(df, col_name, threshold):
  return df[df[col_name] < threshold]

# saving the min and max value for the attribute 'Power_kW'
min_kW = data_de['Power_kW'].min()
max_kW = data_de['Power_kW'].max()
# setting the treshold (10% below and above the extreme values)
threshold_down = min_kW - 0.1 * min_kW
threshold_up = max_kW + 0.1 * max_kW
# creating dataset without outliers
data_out = data_de
data_out = without_outlier_down(data_out, 'Power_kW', threshold_down)
data_out = without_outlier_up(data_out, 'Power_kW', threshold_up)

"""## 4.3. Heat Map
By analizing the heat map, the missing records were detected. All the steps that deal with that are done in Chapter. 3.2. Two different strategies have been carried out in order to compere the results.

 1) Remove all missing data, without any imputations. 
 
 2) Imputting mising records in one year by samples from the other year. 
"""

# Define a new type of plot from scratch
def plot_save_heatmap (df, title_suffix, col_name, y_labels, x_labels, n_reshape, title):
    # Plots a heatmap of data in a column, reshaping the data so that x axis corresponds to length n_measurements
    # The labelling of the axes is taken from the index (rows), and columns
    
    from mpl_toolkits.axes_grid1 import make_axes_locatable
    
    data = df[col_name].values # extract values from selected column (for Heat map to work, we must have a filled matrix!)
    padding_length = n_reshape - len (data) %n_reshape # missing data points for the filled matrix
    data = np.append (data, np.array ([0] * padding_length)) 
        # add missing data points by padding file to reshape with 0. 
        # can use [np.nan] to pad file with NaN instead
    data_shaped = data.reshape ((len(data) // n_reshape, n_reshape)) # reshape to the full matrix; // - integer devision (full numbers)

    y_ticks = data_shaped.shape[0] # get number of yTicks from number of rows (0), or columns - (1)
    min_value = min(data) # min value for colorbar
    max_value = max(data) # max value for colorbar

    fig = plt.figure (figsize = (20,20)) # create and size the figure
    ax = plt.gca() # create an axes instance (not the axis, but the plot line itself - Python convention!)
    im = ax.imshow (data_shaped, vmin = min_value, vmax = max_value, aspect = 'auto', origin = 'upper', cmap = 'magma') 
        # create actual plot, aspect is the ratio of x / y steps, origin - where the data plotting starts, cmap - type of color scheme
    divider = make_axes_locatable (ax) # create a pointer to the legend
    cax = divider.append_axes ("right", size = "3%", pad = 0.2) # specify visual features of legend (= color bar)

    ax.set_title (title + title_suffix, fontdict = {'fontsize':28}) # create plot titles
    ax.set_yticks (range (0, y_ticks, 4)) # create y_ticks every 4 values
    ax.set_yticklabels (labels = y_labels, fontsize = 14) # use list of y_labels created outside function to label y_ticks

    x_interval = n_reshape // len(x_labels) # calculate distance between xticks to match x_labels created outside function
    ax.set_xticks (range (0, n_reshape, x_interval)) # create x_ticks every xInterval values
    ax.set_xticklabels (labels = x_labels, fontsize = 14, rotation = 45, horizontalalignment = 'right')
        # use labels, make them tilted, make the right end of the label align with the x-axis 

    plt.colorbar(im, cax = cax) # create colorbar legend

df_heatmap = data_out.copy (deep=True)
df_heatmap = df_heatmap.set_index ('Date', drop = False)
df_heatmap['WeekDay'] = df_heatmap.index.strftime("%A")

# df_heatmap.head() # view results
## Note: 
    # 1. dataset starts on a Friday, so erase rows in order to start on a Monday
    # 2. 'Week Day' versus 'WeekDay' columns: 1 corresponds to Sunday, and not Monday as was assumed!

# Create labels for X axis
s0 = df_heatmap['WeekDay'].unique().tolist()
s1 = np.repeat(s0, 4) # repeats each element before moving to the next: [Wed, Wed, Wed, etc]
s2 = [' 12 AM ', ' 6 AM', ' 12 PM', ' 6 PM'] # label xTicks every 6 hours
s3 = np.tile(s2, 7) # repeats all elements conserving their order: '12AM ', '6AM', '12PM', '6PM' , '12AM ', '6AM', '12PM', '6PM'
x_names = np.char.add(s1, s3) # creates combination of previous sequences: Mon + 6AM

# Create labels for Y axis
y_start = df_heatmap.index.values [0] # get 1st date from dataframe
y_end =  df_heatmap.index.values [-1] # get last date from dataframe
y_tickLabels = pd.DataFrame (data = pd.date_range (start = y_start, end = y_end, freq = '4W'), columns = ['datetime']) # create sequence of ylabels from start and end date 
y_tickLabels['date'] = y_tickLabels['datetime'].apply (lambda x: x.strftime('%Y-%m-%d')) # set format of labels
y_names = y_tickLabels['date'].tolist() # create list from created names

# Automate the creation of rows, hourly averaged, and daily averaged data and plots
n_reshape = [24 * 7, 7] 
x_names = [x_names, s0]
title_suffixes = ['_Hourly_mean', '_Daily_mean']
dfs = [df_heatmap.resample('H', level = 0).mean(),df_heatmap.resample('D', level = 0).mean()]

for j in range(len(n_reshape)): 
    for i in dfs[j].columns.tolist():
        plot_save_heatmap(df = dfs[j], title_suffix = title_suffixes[j], col_name = i, y_labels = y_names, x_labels = x_names[j], \
                          n_reshape = n_reshape[j], title = i)

"""Outcome:

* df1, df2: complete and periodic - clear pattern in the change of variable in the weekdays betwee 12pm and 6pm.


* meteo: when it comes to weather dataset, there is discountinuity detected. 


The stategy to for missing attributes was done in the first period of the project.

# Clustering
Two different clustering models were used: kMeans and Agglomerative clustering. Clustering was performed for two pairs or features: 'Power_ah'&'WeekDay' and 'Power_Ah'&'Hour', assuming that those two may be grouped because of the obvious impact on the energy consumption. However neither model performed well. Even though, it was assumed ( based on data exploration) that we could get groups of smaller and bigger consumption during some periods, it is not reveiled by clustering methods.
"""

from sklearn.cluster import KMeans
from pandas import DataFrame
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import AgglomerativeClustering
import scipy.cluster.hierarchy as sch

# removing class variables 
data_clust = data_clear.copy()
data_clust = data_clust.drop(['Date','Category_x', 'Category_y'], axis = 1)

# picking columns to cluster
a1 = 'WeekDay'
a2 = 'Hour'
b = 'Power_kW'
df1 = DataFrame(data_clust,columns=[a1, b])
df2 = DataFrame(data_clust,columns=[a2, b])
# normalization
scaler = StandardScaler()
df1_norm = scaler.fit_transform(df1)
df1_norm = pd.DataFrame(df1_norm)
df2_norm = scaler.fit_transform(df2)
df2_norm = pd.DataFrame(df2_norm)

# kmean clustering
kmeans = KMeans(n_clusters = 2)
# first pair of features
kmeans.fit(df1_norm)
y1_km = kmeans.fit_predict(df1_norm)
# second pair of features
kmeans.fit(df2_norm)
y2_km = kmeans.fit_predict(df2_norm)

# agromerative clustering
aglo = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward')
# first pair of features
aglo.fit(df1_norm)
labels1 = aglo.labels_
# second pair of features
aglo.fit(df2_norm)
labels2 = aglo.labels_

# first pair of columns
df1_clust = scaler.inverse_transform(df1_norm)
df1_clust = pd. DataFrame(df1_clust)
df1_clust = df1_clust.rename({0: a1, 1: b}, axis = 1)
# second pair of columns
df2_clust = scaler.inverse_transform(df2_norm)
df2_clust = pd. DataFrame(df2_clust)
df2_clust = df2_clust.rename({0: a2, 1: b}, axis = 1)

# alglomerative clustering
fig, (ax1, ax2) = plt.subplots(1, 2)
fig.set_size_inches(25,7) 

ax1.scatter(df1_clust[a1][labels1==0], df1_clust[b][labels1==0], s=10, marker='o', color='red')
ax1.scatter(df1_clust[a1][labels1==1], df1_clust[b][labels1==1], s=10, marker='o', color='blue')
ax1.scatter(df1_clust[a1][labels1==2], df1_clust[b][labels1==2], s=10, marker='o', color='green')
ax1.scatter(df1_clust[a1][labels1==3], df1_clust[b][labels1==3], s=10, marker='o', color='purple')
ax1.scatter(df1_clust[a1][labels1==4], df1_clust[b][labels1==4], s=10, marker='o', color='orange')
ax1.set(xlabel= a1, ylabel= b)

ax2.scatter(df2_clust[a2][labels2==0], df2_clust[b][labels2==0], s=10, marker='o', color='red')
ax2.scatter(df2_clust[a2][labels2==1], df2_clust[b][labels2==1], s=10, marker='o', color='blue')
ax2.scatter(df2_clust[a2][labels2==2], df2_clust[b][labels2==2], s=10, marker='o', color='green')
ax2.scatter(df2_clust[a2][labels2==3], df2_clust[b][labels2==3], s=10, marker='o', color='purple')
ax2.scatter(df2_clust[a2][labels2==4], df2_clust[b][labels2==4], s=10, marker='o', color='orange')
ax2.set(xlabel= a1, ylabel= b)

# kmeans clustering
fig, (ax1, ax2) = plt.subplots(1, 2)
fig.set_size_inches(25,7) 

ax1.scatter(df1_clust[a1][y1_km == 0], df1_clust[b][y1_km ==0], s=10, c='red')
ax1.scatter(df1_clust[a1][y1_km == 1], df1_clust[b][y1_km ==1], s=10, c='black')
ax1.scatter(df1_clust[a1][y1_km == 2], df1_clust[b][y1_km ==2], s=10, c='green')
ax1.scatter(df1_clust[a1][y1_km == 3], df1_clust[b][y1_km ==3], s=10, c='blue')
ax1.set(xlabel= a1, ylabel= b)

ax2.scatter(df2_clust[a2][y2_km == 0], df2_clust[b][y2_km ==0], s=10, c='red')
ax2.scatter(df2_clust[a2][y2_km == 1], df2_clust[b][y2_km ==1], s=10, c='black')
ax2.scatter(df2_clust[a2][y2_km == 2], df2_clust[b][y2_km ==2], s=10, c='green')
ax2.scatter(df2_clust[a2][y2_km == 3], df2_clust[b][y2_km ==3], s=10, c='blue')
ax2.set(xlabel= a2, ylabel= b)

"""# Feature selection
In this step the instances perfectly correleted with an output were removed (columns with energy, which is besically the same as power). Feature importance was checked. Different methods showed different results, but in general no noisy feature was detected. The weakest seems to be 'windSpeed_m/s', 'rain_mm/h' and 'Category_x'. It was decided to leave all features, since the dataset is not big and the reducement of dimensionality is not required. 
"""

data_sel = data_clear.copy()
# deleting features which are not float
data_sel = data_sel.drop(['Date', 'Category_y'], axis = 1)
# creating a column with Power from previous hour
data_sel['Power-1'] = data_sel['Power_kW'].shift(1)
data_sel = data_sel.dropna()

import seaborn as sns

# creating the correlation matrix
fig = plt.figure(figsize=[12, 12])
corr_mtx = data_sel.corr()
sns.heatmap(corr_mtx, xticklabels=corr_mtx.columns, yticklabels=corr_mtx.columns, annot=True, cmap='Blues')
plt.title('Correlation analysis')
plt.show()

# deleting highly correlated features (inputs)
data_sel = data_sel.drop(['Current1_Ah', 'Current2_Ah', 'Current3_Ah', 'windGust_m/s'], axis = 1)

# 'Total_Ah' also need to be removed (cause this is actually the same as an output)
data_sel = data_sel.drop(['Total_Ah'], axis = 1)

# defining a function to return the position of columns
def column_index(df, query_cols):
    cols = df.columns.values
    sidx = np.argsort(cols)
    return sidx[np.searchsorted(cols,query_cols,sorter=sidx)]

# defining output an inputs
Y = data_sel.values[:, column_index(data_sel, 'Power_kW' )]
X = data_sel.values[:,column_index(data_sel, ['temp_C', 'HR', 'windSpeed_m/s', 'pres_mbar', 'solarRad_W/m2', 'rain_mm/h', 'rain_day', 'Hour', 'Category_x', 'WeekDay', 'Power-1' ])]

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_regression, mutual_info_regression

"""### Filter methods"""

# Filter methods - kBest
features = SelectKBest(k=10, score_func = f_regression)             # Power-1, solarRad_W/m2, WeekDay, HR, temp_C, Hour, rain_day, windSpeed_m/s, pres_mbar, Category_x, rain_mm/h
# features = SelectKBest(k=10, score_func = mutual_info_regression)     # Power-1, Hour, solar_Rad_W/m2, WeekDay, temp_C, HR, pres_mbar, rain_day, Category_x, wind_Speed_m/s, rain_mm/h
fit = features.fit(X,Y)
new_data_kBest = fit.transform(X)
print(fit.scores_)
print(new_data_kBest)

"""### Wrapper methods"""

from sklearn.feature_selection import RFE
from sklearn.linear_model import LinearRegression

model = LinearRegression() # LinearRegression Model as Estimator
rfe = RFE(model, 2)
rfe2 = RFE(model, 5) 
rfe3 = RFE(model, 10) 

fit = rfe.fit(X,Y)
fit2 = rfe2.fit(X,Y)
fit3 = rfe3.fit(X,Y)
# new_data_wrap = fit.transform(X)
# new_data_wrap

print( "Feature Ranking (Linear Model, 2 features): %s" % (fit.ranking_)) # WeekDay, rain_day, pres_mbar, rain_mm/h, solarRad_W/m2, Hour, temp_C, Category_x, Power-1, HR, WindSpeed_m/s
print( "Feature Ranking (Linear Model, 5 features): %s" % (fit2.ranking_)) # pres_mbar, rain_mm/h, rain_day, WeekDay, solarRad_W/m2, Hour, temp_C, Category_x, Power-1, HR, wind_Speed_m/s
print( "Feature Ranking (Linear Model, 10 features): %s" % (fit3.ranking_)) # temp_C, pres_mbar, solarRad_W/m2, rain_mm/h, rain_day, Hour, Category_x, WeekDay, Power-1, HR, Wind_Speed_m/s

"""### Emsemble methods"""

from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor()
model.fit(X, Y)
print(model.feature_importances_) # solarRad_W/m2, Power-1, HR, pres_mbar, temp_C, wind_Speed_m/s, WeekDay, Category_x, Hour, rain_day, rain_mm/h

# Weekday square
data_sel_2 = data_sel.copy()
data_sel_2['WeekDay2']=np.square(data_sel['WeekDay'])                   # sunday-6
data_sel_2.head()

# recurrent
Y1 = data_sel_2.values[:, column_index(data_sel, 'Power_kW' )]
X1 = data_sel_2.values[:,column_index(data_sel, ['Total_Ah', 'temp_C', 'HR', 'windSpeed_m/s', 'pres_mbar', 'solarRad_W/m2', 'rain_mm/h', 'rain_day', 'Hour', 'Category_x', 'WeekDay', 'Power-1', 'WeekDay2' ])]

model = RandomForestRegressor()
model.fit(X1, Y1)
print(model.feature_importances_) #Power-1, solarRa_W/m2, HR, pres_mbar, wind_Speed_m/s, temp_C, WeekDay, WeekDay2, Cateory_x, Hour, rain_day, rain_mm/h

"""# Regression
9 models were performed. All of them perform vey good. The weakest model turned to be a Linear Regression (e = ~11%) and the best Random Forest, Bootstrapping and X/Gradient Boosting (e = ~5%). Two models used normalized data ( SVR and RF_uni). Using normalized data in Random Forest didn't improve the performances.

"""

from sklearn.model_selection import train_test_split
from sklearn import  metrics

# defining a function to return the position of columns
def column_index(df, query_cols):
    cols = df.columns.values
    sidx = np.argsort(cols)
    return sidx[np.searchsorted(cols,query_cols,sorter=sidx)]

data_reg = data_sel.copy()
# defining output and inputs
Y_reg = data_reg.values[:, column_index(data_reg, 'Power_kW' )]
X_reg = data_reg.values[:, column_index(data_reg, ['temp_C', 'HR', 'windSpeed_m/s', 'pres_mbar', 'solarRad_W/m2', 'rain_mm/h', 'rain_day', 'Hour', 'WeekDay', 'Power-1'])]  # if want add 'Category_x' feature

# spliting the data for training and testng sets
X_train, X_test, y_train, y_test = train_test_split(X_reg,Y_reg)    # 75%/25%

scaler = StandardScaler()
# Fit only to the training data
scaler.fit(X_train)
# Now apply the transformations to the data:
X_train_scaled = scaler.transform(X_train)
y_train_scaled = scaler.fit_transform(y_train.reshape(-1,1))

"""## Linear regression"""

from sklearn import  linear_model

# creating linear regression object
regr = linear_model.LinearRegression()
# training the model using the training sets
regr.fit(X_train, y_train)
# Make predictions using the testing set
y_pred_LR = regr.predict(X_test)

fig, (ax1, ax2) = plt.subplots(1, 2)
fig.set_size_inches(25,7) 

ax1.plot(y_test[1:200], color = 'tab:cyan', linewidth = 3)
ax1.plot(y_pred_LR[1:200], color = 'black', linewidth = 2)
ax1.legend(['test', 'predict'])
ax2.scatter(y_test, y_pred_LR, color = 'tab:cyan', linewidths = 0.2)

#Evaluate errors
MAE_LR = metrics.mean_absolute_error(y_test, y_pred_LR) 
MSE_LR = metrics.mean_squared_error(y_test, y_pred_LR)  
RMSE_LR = np.sqrt(metrics.mean_squared_error(y_test, y_pred_LR))
cvRMSE_LR = RMSE_LR/np.mean(y_test)
print('MAE =', MAE_LR, '\nMSE = ',  MSE_LR, '\nRMSE = ', RMSE_LR, '\ncvRMSE = ', cvRMSE_LR)

"""## Support Vector Regression"""

from sklearn.svm import SVR

regr = SVR(kernel='rbf')
regr.fit(X_train_scaled, y_train_scaled)

y_pred_SVR_norm = regr.predict(scaler.fit_transform(X_test))
y_test_SVR_norm = scaler.fit_transform(y_test.reshape(-1,1))
y_pred_SVR = scaler.inverse_transform(y_pred_SVR_norm)
# y_test_SVR = scaler.inverse_transform(y_test_SVR_norm)

fig, (ax1, ax2) = plt.subplots(1, 2)
fig.set_size_inches(25,7) 


ax1.plot(y_test[1:200], color = 'tab:orange', linewidth = 3)
ax1.plot(y_pred_SVR[1:200], color = 'black', linewidth = 2)
ax1.legend(['test', 'predict'])
ax2.scatter(y_test, y_pred_SVR, color = 'tab:orange', linewidths = 0.2)

MAE_SVR = metrics.mean_absolute_error(y_test, y_pred_SVR) 
MSE_SVR = metrics.mean_squared_error(y_test, y_pred_SVR)  
RMSE_SVR = np.sqrt(metrics.mean_squared_error(y_test, y_pred_SVR))
cvRMSE_SVR = RMSE_SVR/np.mean(y_test)
print('MAE =', MAE_SVR, '\nMSE = ',  MSE_SVR, '\nRMSE = ', RMSE_SVR, '\ncvRMSE = ', cvRMSE_SVR)

"""## Regression Decision Tree"""

from sklearn.tree import DecisionTreeRegressor

regr = DecisionTreeRegressor()
regr.fit(X_train, y_train)
y_pred_DT = regr.predict(X_test)

fig, (ax1, ax2) = plt.subplots(1, 2)
fig.set_size_inches(25,7) 

ax1.plot(y_test[1:200], color = 'tab:green', linewidth = 3)
ax1.plot(y_pred_DT[1:200], color = 'black', linewidth = 2)
ax1.legend(['test', 'predict'])
ax2.scatter(y_test, y_pred_DT, color = 'tab:green', linewidths = 0.2)

#Evaluate errors
MAE_DT=metrics.mean_absolute_error(y_test,y_pred_DT) 
MSE_DT=metrics.mean_squared_error(y_test,y_pred_DT)  
RMSE_DT= np.sqrt(metrics.mean_squared_error(y_test,y_pred_DT))
cvRMSE_DT=RMSE_DT/np.mean(y_test)
print('MAE =', MAE_DT, '\nMSE = ',  MSE_DT, '\nRMSE = ', RMSE_DT, '\ncvRMSE = ', cvRMSE_DT)

"""## Random forest"""

from sklearn.ensemble import RandomForestRegressor

RF_model = RandomForestRegressor()
RF_model.fit(X_train, y_train)
y_pred_RF = RF_model.predict(X_test)

fig, (ax1, ax2) = plt.subplots(1, 2)
fig.set_size_inches(25,7) 

ax1.plot(y_test[1:200], color = 'tab:pink', linewidth = 3)
ax1.plot(y_pred_RF[1:200], color = 'black', linewidth = 2)
ax1.legend(['test', 'predict'])
ax2.scatter(y_test, y_pred_RF, color = 'tab:pink', linewidths = 0.2)

#Evaluate errors
MAE_RF=metrics.mean_absolute_error(y_test, y_pred_RF) 
MSE_RF=metrics.mean_squared_error(y_test, y_pred_RF)  
RMSE_RF= np.sqrt(metrics.mean_squared_error(y_test,y_pred_RF))
cvRMSE_RF=RMSE_RF/np.mean(y_test)
print('MAE =', MAE_RF, '\nMSE = ',  MSE_RF, '\nRMSE = ', RMSE_RF, '\ncvRMSE = ', cvRMSE_RF)

"""## Uniformed data

### Random forest uniformed data
"""

parameters_uni = {'bootstrap': True, 'min_samples_leaf': 3, 'n_estimators': 100, 'min_samples_split': 15, 'max_features': 'sqrt', 'max_depth': 10, 'max_leaf_nodes': None}

RF_model_uni = RandomForestRegressor(**parameters_uni)
RF_model_uni.fit(X_train_scaled, y_train.reshape(-1,1))
y_pred_RF_uni = RF_model_uni.predict(scaler.fit_transform(X_test))

fig, (ax1, ax2) = plt.subplots(1, 2)
fig.set_size_inches(25,7) 

ax1.plot(y_test[1:200], color = 'tab:red', linewidth = 3)
ax1.plot(y_pred_RF_uni[1:200], color = 'black', linewidth = 2)
ax1.legend(['test', 'predict'])
ax2.scatter(y_test, y_pred_RF_uni, color = 'tab:red', linewidths = 0.2)

#Evaluate errors
MAE_RF_uni = metrics.mean_absolute_error(y_test, y_pred_RF_uni) 
MSE_RF_uni = metrics.mean_squared_error(y_test, y_pred_RF_uni)  
RMSE_RF_uni = np.sqrt(metrics.mean_squared_error(y_test, y_pred_RF_uni))
cvRMSE_RF_uni = RMSE_RF_uni/np.mean(y_test)
print('MAE =', MAE_RF_uni, '\nMSE = ',  MSE_RF_uni, '\nRMSE = ', RMSE_RF_uni, '\ncvRMSE = ', cvRMSE_RF_uni)

"""## Gradient Boosting"""

from sklearn.ensemble import GradientBoostingRegressor

#params = {'n_estimators': 500, 'max_depth': 4, 'min_samples_split': 2,
#          'learning_rate': 0.01, 'loss': 'ls'}
#GB_model = GradientBoostingRegressor(**params)

GB_model = GradientBoostingRegressor()
GB_model.fit(X_train, y_train)
y_pred_GB =GB_model.predict(X_test)

fig, (ax1, ax2) = plt.subplots(1, 2)
fig.set_size_inches(25,7) 

ax1.plot(y_test[1:200], color = 'tab:purple', linewidth = 3)
ax1.plot(y_pred_GB[1:200], color = 'black', linewidth = 2)
ax1.legend(['test', 'predict'])
ax2.scatter(y_test, y_pred_GB, color = 'tab:purple', linewidths = 0.2)

#Evaluate errors
MAE_GB = metrics.mean_absolute_error(y_test, y_pred_GB) 
MSE_GB = metrics.mean_squared_error(y_test, y_pred_GB)  
RMSE_GB = np.sqrt(metrics.mean_squared_error(y_test, y_pred_GB))
cvRMSE_GB = RMSE_GB/np.mean(y_test)
print('MAE =', MAE_GB, '\nMSE = ',  MSE_GB, '\nRMSE = ', RMSE_GB, '\ncvRMSE = ', cvRMSE_GB)

"""## Extreme Gradient Boosting"""

from xgboost import XGBRegressor

#params = {'n_estimators': 500, 'max_depth': 4, 'min_samples_split': 2,
#          'learning_rate': 0.01, 'loss': 'ls'}
#GB_model = GradientBoostingRegressor(**params)

XGB_model = XGBRegressor()
XGB_model.fit(X_train, y_train)
y_pred_XGB =XGB_model.predict(X_test)

fig, (ax1, ax2) = plt.subplots(1, 2)
fig.set_size_inches(25,7) 

ax1.plot(y_test[1:200], color = 'tab:blue', linewidth = 3)
ax1.plot(y_pred_XGB[1:200], color = 'black', linewidth = 2)
ax1.legend(['test', 'predict'])
ax2.scatter(y_test, y_pred_XGB, color = 'tab:blue', linewidths = 0.2)

#Evaluate errors
MAE_XGB = metrics.mean_absolute_error(y_test, y_pred_XGB) 
MSE_XGB = metrics.mean_squared_error(y_test, y_pred_XGB)  
RMSE_XGB = np.sqrt(metrics.mean_squared_error(y_test, y_pred_XGB))
cvRMSE_XGB = RMSE_XGB/np.mean(y_test)
print('MAE =', MAE_XGB, '\nMSE = ',  MSE_XGB, '\nRMSE = ', RMSE_XGB, '\ncvRMSE = ', cvRMSE_XGB)

"""## Bootsrapping"""

from sklearn.ensemble import BaggingRegressor

BT_model = BaggingRegressor()
BT_model.fit(X_train, y_train)
y_pred_BT =BT_model.predict(X_test)

fig, (ax1, ax2) = plt.subplots(1, 2)
fig.set_size_inches(25,7) 

ax1.plot(y_test[1:200], color = 'tab:brown', linewidth = 3)
ax1.plot(y_pred_BT[1:200], color = 'black', linewidth = 2)
ax1.legend(['test', 'predict'])
ax2.scatter(y_test, y_pred_BT, color = 'tab:brown', linewidths = 0.2)

#Evaluate errors
MAE_BT = metrics.mean_absolute_error(y_test, y_pred_BT) 
MSE_BT = metrics.mean_squared_error(y_test, y_pred_BT)  
RMSE_BT = np.sqrt(metrics.mean_squared_error(y_test, y_pred_BT))
cvRMSE_BT = RMSE_BT/np.mean(y_test)
print('MAE =', MAE_BT, '\nMSE = ',  MSE_BT, '\nRMSE = ', RMSE_BT, '\ncvRMSE = ', cvRMSE_BT)

"""## Neural Network"""

from sklearn.neural_network import MLPRegressor

NN_model = MLPRegressor(hidden_layer_sizes=(20,20,20))
NN_model.fit(X_train,y_train)
y_pred_NN = NN_model.predict(X_test)

fig, (ax1, ax2) = plt.subplots(1, 2)
fig.set_size_inches(25,7) 

ax1.plot(y_test[1:200], color = 'gold', linewidth = 3)
ax1.plot(y_pred_NN[1:200], color = 'black', linewidth = 2)
ax1.legend(['test', 'predict'])
ax2.scatter(y_test, y_pred_NN, color = 'gold', linewidths = 0.2)

#Evaluate errors
MAE_NN = metrics.mean_absolute_error(y_test, y_pred_NN) 
MSE_NN = metrics.mean_squared_error(y_test, y_pred_NN)  
RMSE_NN = np.sqrt(metrics.mean_squared_error(y_test, y_pred_NN))
cvRMSE_NN = RMSE_NN/np.mean(y_test)
print('MAE =', MAE_NN, '\nMSE = ',  MSE_NN, '\nRMSE = ', RMSE_NN, '\ncvRMSE = ', cvRMSE_NN)

"""## Error Metrics"""

metrics1 = {'LR':[MAE_LR, MSE_LR, RMSE_LR, cvRMSE_LR],
        'SVR':[MAE_SVR, MSE_SVR, RMSE_SVR, cvRMSE_SVR],
        'DT':[MAE_DT, MSE_DT, RMSE_DT, cvRMSE_DT],
        'RF':[MAE_RF, MSE_RF, RMSE_RF, cvRMSE_RF],
        'RF uni':[MAE_RF_uni, MSE_RF_uni, RMSE_RF_uni, cvRMSE_RF_uni],} 
errors1 = pd.DataFrame(metrics1, index =['MAE', 'MSE', 'RMSE', 'cvRMSE']) 

metrics2 = {'GB':[MAE_GB, MSE_GB, RMSE_GB, cvRMSE_GB],
        'XGB':[MAE_XGB, MSE_XGB, RMSE_XGB, cvRMSE_XGB],
        'BT':[MAE_BT, MSE_BT, RMSE_BT, cvRMSE_BT],
        'NN':[MAE_NN, MSE_NN, RMSE_NN, cvRMSE_NN]} 
errors2 = pd.DataFrame(metrics2, index =['MAE', 'MSE', 'RMSE', 'cvRMSE']) 

error = pd.concat([errors1, errors2], axis=1)

error

"""# Comparison of results
The models were run for 4 different datasets in order to compare the impact of changes:

1) dataset with not full weather data (with 'Category_x' feature)

2) dataset with imputed weather data (with 'Category_x' feature)

3) dataset with not full weather data (without 'Category_x' feature)

4) dataset with imputed weather data (without 'Category_x' feature)

Feature 'Category_x' distinguish the periods of holidays, classes and exams.

Using the bigger weather data (artificially imputed) result in smaller error by 0,5% (in weker models like LR and SVR) and around 1% in other models. 
Using a feature 'Category_x', which stands for holidays, classes and exams periods do not improve the error and even in some models make predictions worse, thus it is suggested not to use it (the quality of that information is very doubtful).
"""

# # saiving errors to csv file
# error.to_csv(r'/content/drive/My Drive/Colab Notebooks/Energy Services IST/errors_meteo_complete.csv', index = False)

"""### Conclusions:
 
"""

path01 = '/content/drive/My Drive/Colab Notebooks/Energy Services IST/errors_meteo_not_complete.csv'
not_complete = pd.read_csv(path01)
path02 = '/content/drive/My Drive/Colab Notebooks/Energy Services IST/errors_meteo_complete.csv'
complete = pd.read_csv(path02)
path03 = '/content/drive/My Drive/Colab Notebooks/Energy Services IST/errors_meteo_not_complete_no_category.csv'
not_complete_no_cat = pd.read_csv(path03)
path04 = '/content/drive/My Drive/Colab Notebooks/Energy Services IST/errors_meteo_complete_no_category.csv'
complete_no_cat = pd.read_csv(path04)

not_complete = not_complete.rename({0: 'MAE', 1: 'MSE', 2: 'RMSE', 3: 'cvRMSE'}, axis = 0)
complete = complete.rename({0: 'MAE', 1: 'MSE', 2: 'RMSE', 3: 'cvRMSE'}, axis = 0)
not_complete_no_cat = not_complete_no_cat.rename({0: 'MAE', 1: 'MSE', 2: 'RMSE', 3: 'cvRMSE'}, axis = 0)
complete_no_cat = complete_no_cat.rename({0: 'MAE', 1: 'MSE', 2: 'RMSE', 3: 'cvRMSE'}, axis = 0)

from tabulate import tabulate

pdtab=lambda df:tabulate(df,headers='keys',tablefmt='psql')
print('NOT COMPLETE with category \n',pdtab(not_complete))
print('COMPLETE with category \n',pdtab(complete))
print('NOT COMPLETE without category \n', pdtab(not_complete_no_cat))
print('COMPLETE without category \n', pdtab(complete_no_cat))